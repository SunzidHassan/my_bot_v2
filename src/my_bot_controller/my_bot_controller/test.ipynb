{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hugginface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grounding DINO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection \n",
    "\n",
    "model_id = \"IDEA-Research/grounding-dino-base\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "model = AutoModelForZeroShotObjectDetection.from_pretrained(model_id).to(device)\n",
    "\n",
    "image_url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(image_url, stream=True).raw)\n",
    "# Check for cats and remote controls\n",
    "# VERY important: text queries need to be lowercased + end with a dot\n",
    "text = \"a cat.\"\n",
    "\n",
    "inputs = processor(images=image, text=text, return_tensors=\"pt\").to(device)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "results = processor.post_process_grounded_object_detection(\n",
    "    outputs,\n",
    "    inputs.input_ids,\n",
    "    box_threshold=0.4,\n",
    "    text_threshold=0.3,\n",
    "    target_sizes=[image.size[::-1]]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'scores': tensor([0.4411], device='cuda:0'),\n",
       "  'labels': ['a cat'],\n",
       "  'boxes': tensor([[ 10.1677,  52.5223, 316.9800, 473.0129]], device='cuda:0')}]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bounding box coordinates\n",
    "x_min, y_min, x_max, y_max = results[0]['boxes'][0].tolist()\n",
    "box_width = x_max - x_min\n",
    "box_height = y_max - y_min\n",
    "bounding_box_area = box_width * box_height\n",
    "\n",
    "# Image dimensions and area\n",
    "image_width, image_height = image.size\n",
    "image_area = image_width * image_height\n",
    "\n",
    "# Ratio of bounding box area to image area\n",
    "area_ratio = round(bounding_box_area / image_area, 2)\n",
    "area_ratio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Florence 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, AutoModelForCausalLM  \n",
    "from PIL import Image, ImageDraw, ImageFont \n",
    "import requests\n",
    "import copy\n",
    "import matplotlib.pyplot as plt  \n",
    "import matplotlib.patches as patches  \n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install einops timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_id = 'microsoft/Florence-2-large'\n",
    "model_id = 'microsoft/Florence-2-base'\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True).eval().cuda()\n",
    "processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_example(task_prompt, text_input=None):\n",
    "    if text_input is None:\n",
    "        prompt = task_prompt\n",
    "    else:\n",
    "        prompt = task_prompt + text_input\n",
    "    inputs = processor(text=prompt, images=image, return_tensors=\"pt\")\n",
    "    generated_ids = model.generate(\n",
    "      input_ids=inputs[\"input_ids\"].cuda(),\n",
    "      pixel_values=inputs[\"pixel_values\"].cuda(),\n",
    "      max_new_tokens=1024,\n",
    "      early_stopping=False,\n",
    "      do_sample=False,\n",
    "      num_beams=3,\n",
    "    )\n",
    "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n",
    "    parsed_answer = processor.post_process_generation(\n",
    "        generated_text, \n",
    "        task=task_prompt, \n",
    "        image_size=(image.width, image.height)\n",
    "    )\n",
    "\n",
    "    return parsed_answer\n",
    "\n",
    "def plot_bbox(image, data):\n",
    "   # Create a figure and axes  \n",
    "    fig, ax = plt.subplots()  \n",
    "      \n",
    "    # Display the image  \n",
    "    ax.imshow(image)  \n",
    "      \n",
    "    # Plot each bounding box  \n",
    "    for bbox, label in zip(data['bboxes'], data['labels']):  \n",
    "        # Unpack the bounding box coordinates  \n",
    "        x1, y1, x2, y2 = bbox  \n",
    "        # Create a Rectangle patch  \n",
    "        rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=1, edgecolor='r', facecolor='none')  \n",
    "        # Add the rectangle to the Axes  \n",
    "        ax.add_patch(rect)  \n",
    "        # Annotate the label  \n",
    "        plt.text(x1, y1, label, color='white', fontsize=8, bbox=dict(facecolor='red', alpha=0.5))  \n",
    "      \n",
    "    # Remove the axis ticks and labels  \n",
    "    ax.axis('off')  \n",
    "      \n",
    "    # Show the plot  \n",
    "    plt.show()\n",
    "\n",
    "colormap = ['indigo']\n",
    "# colormap = ['blue','orange','green','purple','brown','pink','gray','olive','cyan','red',\n",
    "            # 'lime','indigo','violet','aqua','magenta','coral','gold','tan','skyblue']\n",
    "\n",
    "def draw_polygons(image, prediction, fill_mask=False):  \n",
    "    \"\"\"  \n",
    "    Draws segmentation masks with polygons on an image.  \n",
    "  \n",
    "    Parameters:  \n",
    "    - image_path: Path to the image file.  \n",
    "    - prediction: Dictionary containing 'polygons' and 'labels' keys.  \n",
    "                  'polygons' is a list of lists, each containing vertices of a polygon.  \n",
    "                  'labels' is a list of labels corresponding to each polygon.  \n",
    "    - fill_mask: Boolean indicating whether to fill the polygons with color.  \n",
    "    \"\"\"  \n",
    "    # Load the image  \n",
    "   \n",
    "    draw = ImageDraw.Draw(image)  \n",
    "      \n",
    "   \n",
    "    # Set up scale factor if needed (use 1 if not scaling)  \n",
    "    scale = 1  \n",
    "      \n",
    "    # Iterate over polygons and labels  \n",
    "    for polygons, label in zip(prediction['polygons'], prediction['labels']):  \n",
    "        color = random.choice(colormap)  \n",
    "        fill_color = random.choice(colormap) if fill_mask else None  \n",
    "          \n",
    "        for _polygon in polygons:  \n",
    "            _polygon = np.array(_polygon).reshape(-1, 2)  \n",
    "            if len(_polygon) < 3:  \n",
    "                print('Invalid polygon:', _polygon)  \n",
    "                continue  \n",
    "              \n",
    "            _polygon = (_polygon * scale).reshape(-1).tolist()  \n",
    "              \n",
    "            # Draw the polygon  \n",
    "            if fill_mask:  \n",
    "                draw.polygon(_polygon, outline=color, fill=fill_color)  \n",
    "            else:  \n",
    "                draw.polygon(_polygon, outline=color)  \n",
    "              \n",
    "            # Draw the label text  \n",
    "            draw.text((_polygon[0] + 8, _polygon[1] + 2), label, fill=color)  \n",
    "  \n",
    "    # Save or display the image  \n",
    "    #image.show()  # Display the image  \n",
    "    display(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Phrase grounding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from PIL import Image\n",
    "\n",
    "image_url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image = Image.open(requests.get(image_url, stream=True).raw)\n",
    "image = image.resize((64, 64))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<CAPTION_TO_PHRASE_GROUNDING>': {'bboxes': [[3.680000066757202, 9.184000015258789, 17.88800048828125, 15.904000282287598]], 'labels': ['a remote control']}}\n"
     ]
    }
   ],
   "source": [
    "task_prompt = '<CAPTION_TO_PHRASE_GROUNDING>'\n",
    "results = run_example(task_prompt, text_input=\"a remote control.\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_bbox(image, results['<CAPTION_TO_PHRASE_GROUNDING>'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'<REFERRING_EXPRESSION_SEGMENTATION>': {'polygons': [[[36.57600021362305, 13.02400016784668, 37.02400207519531, 12.064000129699707, 37.7920036315918, 11.040000915527344, 38.560001373291016, 9.056000709533691, 40.03200149536133, 7.072000503540039, 41.56800079345703, 5.600000381469727, 42.52800369262695, 4.896000385284424, 44.000003814697266, 4.448000431060791, 45.53600311279297, 4.448000431060791, 47.07200241088867, 3.4240002632141113, 48.608001708984375, 3.4240002632141113, 50.08000183105469, 3.872000217437744, 51.10400390625, 4.448000431060791, 51.87200164794922, 4.896000385284424, 52.6400032043457, 5.600000381469727, 53.40800094604492, 6.368000507354736, 54.112003326416016, 6.368000507354736, 54.880001068115234, 7.072000503540039, 55.58400344848633, 7.072000503540039, 56.35200119018555, 7.648000240325928, 57.12000274658203, 8.35200023651123, 58.336002349853516, 8.35200023651123, 59.10400390625, 9.056000709533691, 59.87200164794922, 9.824000358581543, 60.6400032043457, 9.824000358581543, 61.60000228881836, 11.040000915527344, 62.62400436401367, 11.040000915527344, 63.96800231933594, 12.064000129699707, 63.96800231933594, 13.53600025177002, 62.62400436401367, 13.53600025177002, 61.60000228881836, 13.02400016784668, 60.6400032043457, 13.02400016784668, 60.6400032043457, 14.04800033569336, 61.60000228881836, 15.008000373840332, 62.62400436401367, 15.008000373840332, 63.96800231933594, 16.288000106811523, 63.96800231933594, 17.760000228881836, 62.62400436401367, 17.760000228881836, 61.60000228881836, 17.312000274658203, 60.6400032043457, 16.288000106811523, 59.87200164794922, 16.288000106811523, 59.10400390625, 15.712000846862793, 58.336002349853516, 15.008000373840332, 57.12000274658203, 15.008000373840332, 56.35200119018555, 15.520000457763672, 55.58400344848633, 16.288000106811523, 54.880001068115234, 16.288000106811523, 54.112003326416016, 17.312000274658203, 53.40800094604492, 18.784000396728516, 53.40800094604492, 19.744001388549805, 52.6400032043457, 21.216001510620117, 52.6400032043457, 22.240001678466797, 51.87200164794922, 23.200000762939453, 51.10400390625, 24.672000885009766, 50.336002349853516, 25.696001052856445, 49.88800048828125, 26.656002044677734, 49.44000244140625, 28.128002166748047, 49.44000244140625, 29.60000228881836, 49.88800048828125, 30.36800193786621, 51.10400390625, 31.840002059936523, 51.87200164794922, 33.3120002746582, 52.6400032043457, 34.784000396728516, 54.112003326416016, 36.57600021362305, 54.880001068115234, 37.02400207519531, 57.12000274658203, 37.02400207519531, 57.888004302978516, 38.04800033569336, 57.888004302978516, 39.00800323486328, 57.12000274658203, 40.03200149536133, 54.112003326416016, 40.03200149536133, 53.40800094604492, 39.00800323486328, 52.6400032043457, 39.00800323486328, 52.6400032043457, 40.03200149536133, 53.40800094604492, 41.50400161743164, 54.112003326416016, 42.784000396728516, 54.880001068115234, 44.25600051879883, 56.35200119018555, 45.7920036315918, 56.8640022277832, 46.816001892089844, 56.8640022277832, 48.288002014160156, 56.35200119018555, 48.80000305175781, 54.880001068115234, 48.80000305175781, 54.112003326416016, 48.288002014160156, 53.40800094604492, 46.816001892089844, 52.6400032043457, 45.7920036315918, 51.87200164794922, 44.25600051879883, 51.10400390625, 42.784000396728516, 50.336002349853516, 41.50400161743164, 48.8640022277832, 40.03200149536133, 48.09600067138672, 39.00800323486328, 47.07200241088867, 40.03200149536133, 46.30400085449219, 41.50400161743164, 46.30400085449219, 42.784000396728516, 45.53600311279297, 44.25600051879883, 45.53600311279297, 45.02400207519531, 44.768001556396484, 45.7920036315918, 44.000003814697266, 45.7920036315918, 42.52800369262695, 46.816001892089844, 41.56800079345703, 46.816001892089844, 38.560001373291016, 45.7920036315918, 37.02400207519531, 45.7920036315918, 35.80800247192383, 46.560001373291016, 35.040000915527344, 45.7920036315918, 35.80800247192383, 45.02400207519531, 36.25600051879883, 44.25600051879883, 36.25600051879883, 40.480003356933594, 35.80800247192383, 40.03200149536133, 35.42399978637695, 38.04800033569336, 35.42399978637695, 36.57600021362305, 35.80800247192383, 36.12800216674805, 36.25600051879883, 33.12000274658203, 35.80800247192383, 29.60000228881836, 35.42399978637695, 29.152002334594727, 35.040000915527344, 27.61600112915039, 35.040000915527344, 20.768001556396484, 35.42399978637695, 17.760000228881836, 35.80800247192383, 17.312000274658203, 36.25600051879883, 15.520000457763672, 36.25600051879883, 14.04800033569336]]], 'labels': ['']}}\n"
     ]
    }
   ],
   "source": [
    "task_prompt = '<REFERRING_EXPRESSION_SEGMENTATION>'\n",
    "results = run_example(task_prompt, text_input=\"a cat\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_polygon_area(polygon_points):\n",
    "    \"\"\"\n",
    "    Calculate the area of a polygon using the Shoelace formula (also known as surveyor's formula).\n",
    "    \n",
    "    Args:\n",
    "        polygon_points (list): List of [x, y] coordinates defining the polygon vertices\n",
    "    \n",
    "    Returns:\n",
    "        float: Area of the polygon\n",
    "    \"\"\"\n",
    "    n = len(polygon_points)\n",
    "    area = 0.0\n",
    "    \n",
    "    for i in range(n):\n",
    "        j = (i + 1) % n\n",
    "        area += polygon_points[i][0] * polygon_points[j][1]\n",
    "        area -= polygon_points[j][0] * polygon_points[i][1]\n",
    "    \n",
    "    area = abs(area) / 2.0\n",
    "    return area\n",
    "\n",
    "def get_segmentation_ratio(segmentation_results, image_width=640, image_height=480):\n",
    "    \"\"\"\n",
    "    Calculate the ratio of segmented area to total image area.\n",
    "    \n",
    "    Args:\n",
    "        segmentation_results (d ict): Dictionary containing segmentation polygons\n",
    "        image_width (int): Width of the image (default 640 based on environment)\n",
    "        image_height (int): Height of the image (default 480 based on environment)\n",
    "    \n",
    "    Returns:\n",
    "        float: Ratio of segmented area to total image area\n",
    "    \"\"\"\n",
    "    # Extract polygon points\n",
    "    polygons = segmentation_results['<REFERRING_EXPRESSION_SEGMENTATION>']['polygons']\n",
    "    \n",
    "    # Calculate total segmented area (sum of all polygon areas)\n",
    "    total_segmented_area = sum(calculate_polygon_area(polygon) for polygon in polygons)\n",
    "    \n",
    "    # Calculate total image area\n",
    "    total_image_area = image_width * image_height\n",
    "    \n",
    "    # Calculate and return ratio\n",
    "    ratio = round(total_segmented_area / total_image_area, 4)\n",
    "    \n",
    "    return ratio\n",
    "\n",
    "# Example usage with your results\n",
    "ratio = get_segmentation_ratio(results)\n",
    "print(f\"Segmentation area ratio: {ratio}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bounding box coordinates\n",
    "x_min, y_min, x_max, y_max = results[0]['boxes'][0].tolist()\n",
    "box_width = x_max - x_min\n",
    "box_height = y_max - y_min\n",
    "bounding_box_area = box_width * box_height\n",
    "\n",
    "# Image dimensions and area\n",
    "image_width, image_height = image.size\n",
    "image_area = image_width * image_height\n",
    "\n",
    "# Ratio of bounding box area to image area\n",
    "area_ratio = round(bounding_box_area / image_area, 2)\n",
    "area_ratio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=64x64 at 0x7EE70DF425C0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output_image = copy.deepcopy(image)\n",
    "draw_polygons(output_image, results['<REFERRING_EXPRESSION_SEGMENTATION>'], fill_mask=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, AutoModelForCausalLM  \n",
    "from PIL import Image, ImageDraw \n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Initialize the model and processor\n",
    "model_id = 'microsoft/Florence-2-base'\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True).eval().cuda()\n",
    "processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True)\n",
    "\n",
    "def run_example(task_prompt, image, text_input=None):\n",
    "    \"\"\"Generates segmentation results based on the input image and task prompt.\"\"\"\n",
    "    if text_input is None:\n",
    "        prompt = task_prompt\n",
    "    else:\n",
    "        prompt = task_prompt + text_input\n",
    "    inputs = processor(text=prompt, images=image, return_tensors=\"pt\")\n",
    "    generated_ids = model.generate(\n",
    "      input_ids=inputs[\"input_ids\"].cuda(),\n",
    "      pixel_values=inputs[\"pixel_values\"].cuda(),\n",
    "      max_new_tokens=1024,\n",
    "      early_stopping=False,\n",
    "      do_sample=False,\n",
    "      num_beams=3,\n",
    "    )\n",
    "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]\n",
    "    parsed_answer = processor.post_process_generation(\n",
    "        generated_text, \n",
    "        task=task_prompt, \n",
    "        image_size=(image.width, image.height)\n",
    "    )\n",
    "\n",
    "    return parsed_answer\n",
    "\n",
    "colormap = ['indigo']\n",
    "\n",
    "def draw_polygons(image, prediction, fill_mask=False):  \n",
    "    \"\"\"Draws segmentation masks with polygons on an image and returns the modified image.\"\"\"\n",
    "    draw = ImageDraw.Draw(image)\n",
    "\n",
    "    for polygons, label in zip(prediction['polygons'], prediction['labels']):  \n",
    "        color = random.choice(colormap)  \n",
    "        fill_color = random.choice(colormap) if fill_mask else None  \n",
    "          \n",
    "        for _polygon in polygons:  \n",
    "            _polygon = np.array(_polygon).reshape(-1, 2)  \n",
    "            if len(_polygon) < 3:  \n",
    "                print('Invalid polygon:', _polygon)  \n",
    "                continue  \n",
    "              \n",
    "            _polygon = (_polygon * 1).reshape(-1).tolist()  # Adjust for scale if needed\n",
    "              \n",
    "            # Draw the polygon  \n",
    "            if fill_mask:  \n",
    "                draw.polygon(_polygon, outline=color, fill=fill_color)  \n",
    "            else:  \n",
    "                draw.polygon(_polygon, outline=color)  \n",
    "              \n",
    "            # Draw the label text  \n",
    "            draw.text((_polygon[0] + 8, _polygon[1] + 2), label, fill=color)  \n",
    "  \n",
    "    return image  # Return the modified image\n",
    "\n",
    "def segment_image(image_input: Image.Image, text_input=\"a cat\") -> Image.Image:\n",
    "    \"\"\"Segments a given PIL Image based on the specified text input and returns the segmented image.\"\"\"\n",
    "    # Resize the image if necessary\n",
    "    image = image_input.resize((64, 64))\n",
    "\n",
    "    task_prompt = '<REFERRING_EXPRESSION_SEGMENTATION>'\n",
    "    results = run_example(task_prompt, image, text_input=text_input)\n",
    "    \n",
    "    output_image = draw_polygons(image.copy(), results['<REFERRING_EXPRESSION_SEGMENTATION>'], fill_mask=True)\n",
    "    \n",
    "    return output_image  # Return the segmented image\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Load an image using PIL\n",
    "    image_path = \"path/to/your/image.jpg\"  # Replace with your image path\n",
    "    image = Image.open(image_path)\n",
    "    \n",
    "    # Get the segmented image\n",
    "    segmented_image = segment_image(image, text_input=\"a cat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=64x64 at 0x7EE70F94B880>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segmented_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torchrl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1+cu121\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyntcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: numpy 1.22.4\n",
      "Uninstalling numpy-1.22.4:\n",
      "  Successfully uninstalled numpy-1.22.4\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting numpy==1.22.4\n",
      "  Using cached numpy-1.22.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n",
      "Installing collected packages: numpy\n",
      "Successfully installed numpy-1.22.4\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in /home/ubuntu/.local/lib/python3.10/site-packages (2.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ubuntu/.local/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /home/ubuntu/.local/lib/python3.10/site-packages (from pandas) (1.22.4)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ubuntu/.local/lib/python3.10/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tqdm in /home/ubuntu/.local/lib/python3.10/site-packages (4.66.5)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tensordict-nightly in /home/ubuntu/.local/lib/python3.10/site-packages (2024.10.21)\n",
      "Requirement already satisfied: cloudpickle in /home/ubuntu/.local/lib/python3.10/site-packages (from tensordict-nightly) (3.1.0)\n",
      "Requirement already satisfied: torch>=2.5.0.dev in /usr/local/lib/python3.10/dist-packages (from tensordict-nightly) (2.5.0)\n",
      "Requirement already satisfied: orjson in /home/ubuntu/.local/lib/python3.10/site-packages (from tensordict-nightly) (3.10.10)\n",
      "Requirement already satisfied: numpy in /home/ubuntu/.local/lib/python3.10/site-packages (from tensordict-nightly) (1.22.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.5.0.dev->tensordict-nightly) (2024.10.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.5.0.dev->tensordict-nightly) (3.16.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.5.0.dev->tensordict-nightly) (3.4.2)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch>=2.5.0.dev->tensordict-nightly) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch>=2.5.0.dev->tensordict-nightly) (12.4.127)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.5.0.dev->tensordict-nightly) (4.12.2)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.5.0.dev->tensordict-nightly) (1.13.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.5.0.dev->tensordict-nightly) (3.1.4)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.10/dist-packages (from torch>=2.5.0.dev->tensordict-nightly) (2.21.5)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch>=2.5.0.dev->tensordict-nightly) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch>=2.5.0.dev->tensordict-nightly) (12.4.127)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.10/dist-packages (from torch>=2.5.0.dev->tensordict-nightly) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.10/dist-packages (from torch>=2.5.0.dev->tensordict-nightly) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch>=2.5.0.dev->tensordict-nightly) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch>=2.5.0.dev->tensordict-nightly) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.10/dist-packages (from torch>=2.5.0.dev->tensordict-nightly) (12.3.1.170)\n",
      "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.5.0.dev->tensordict-nightly) (3.1.0)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.10/dist-packages (from torch>=2.5.0.dev->tensordict-nightly) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.10/dist-packages (from torch>=2.5.0.dev->tensordict-nightly) (11.6.1.9)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.5.0.dev->tensordict-nightly) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.5.0.dev->tensordict-nightly) (3.0.2)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torchrl-nightly in /home/ubuntu/.local/lib/python3.10/site-packages (2024.10.21)\n",
      "Requirement already satisfied: packaging in /usr/lib/python3/dist-packages (from torchrl-nightly) (21.3)\n",
      "Requirement already satisfied: torch>=2.5.0.dev in /usr/local/lib/python3.10/dist-packages (from torchrl-nightly) (2.5.0)\n",
      "Requirement already satisfied: tensordict-nightly in /home/ubuntu/.local/lib/python3.10/site-packages (from torchrl-nightly) (2024.10.21)\n",
      "Requirement already satisfied: numpy in /home/ubuntu/.local/lib/python3.10/site-packages (from torchrl-nightly) (1.22.4)\n",
      "Requirement already satisfied: cloudpickle in /home/ubuntu/.local/lib/python3.10/site-packages (from torchrl-nightly) (3.1.0)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch>=2.5.0.dev->torchrl-nightly) (12.4.127)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.5.0.dev->torchrl-nightly) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.10/dist-packages (from torch>=2.5.0.dev->torchrl-nightly) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.10/dist-packages (from torch>=2.5.0.dev->torchrl-nightly) (12.4.5.8)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.5.0.dev->torchrl-nightly) (3.16.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.10/dist-packages (from torch>=2.5.0.dev->torchrl-nightly) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch>=2.5.0.dev->torchrl-nightly) (12.4.127)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.5.0.dev->torchrl-nightly) (3.4.2)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch>=2.5.0.dev->torchrl-nightly) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.5.0.dev->torchrl-nightly) (3.1.0)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.5.0.dev->torchrl-nightly) (2024.10.0)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.10/dist-packages (from torch>=2.5.0.dev->torchrl-nightly) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch>=2.5.0.dev->torchrl-nightly) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.10/dist-packages (from torch>=2.5.0.dev->torchrl-nightly) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch>=2.5.0.dev->torchrl-nightly) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch>=2.5.0.dev->torchrl-nightly) (12.4.127)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.5.0.dev->torchrl-nightly) (1.13.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.10/dist-packages (from torch>=2.5.0.dev->torchrl-nightly) (2.21.5)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.5.0.dev->torchrl-nightly) (4.12.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.5.0.dev->torchrl-nightly) (1.3.0)\n",
      "Requirement already satisfied: orjson in /home/ubuntu/.local/lib/python3.10/site-packages (from tensordict-nightly->torchrl-nightly) (3.10.10)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.5.0.dev->torchrl-nightly) (3.0.2)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torchvision in /home/ubuntu/.local/lib/python3.10/site-packages (0.20.0)\n",
      "Requirement already satisfied: numpy in /home/ubuntu/.local/lib/python3.10/site-packages (from torchvision) (1.22.4)\n",
      "Requirement already satisfied: torch==2.5.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.5.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/lib/python3/dist-packages (from torchvision) (9.0.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.0->torchvision) (11.2.1.3)\n",
      "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.0->torchvision) (3.1.0)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.5.0->torchvision) (2024.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.0->torchvision) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.0->torchvision) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.0->torchvision) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.0->torchvision) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.0->torchvision) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.0->torchvision) (12.4.127)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.0->torchvision) (3.1.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.0->torchvision) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.0->torchvision) (12.3.1.170)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.5.0->torchvision) (3.4.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.0->torchvision) (12.4.127)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.0->torchvision) (4.12.2)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.0->torchvision) (1.13.1)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.0->torchvision) (12.4.5.8)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.5.0->torchvision) (3.16.1)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.0->torchvision) (12.4.127)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch==2.5.0->torchvision) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.5.0->torchvision) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "# first time installs\n",
    "!pip uninstall numpy -y\n",
    "!pip install numpy==1.22.4\n",
    "!pip install pandas\n",
    "!pip install tqdm\n",
    "!pip install tensordict-nightly\n",
    "!pip install torchrl-nightly\n",
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/torchrl/data/replay_buffers/samplers.py:37: UserWarning: Failed to import torchrl C++ binaries. Some modules (eg, prioritized replay buffers) may not work with your installation. If you installed TorchRL from PyPI, please report the bug on TorchRL github. If you installed TorchRL locally and/or in development mode, check that you have all the required compiling packages.\n",
      "  warnings.warn(EXTENSION_WARNING)\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "# import cv2 as cv\n",
    "import numpy as np\n",
    "# import pandas as pd\n",
    "import torch\n",
    "import tqdm\n",
    "from tensordict import TensorDict, TensorDictBase\n",
    "from tensordict.nn import TensorDictModule\n",
    "from torch import nn\n",
    "import torchvision\n",
    "from torchrl.data import BoundedTensorSpec, CompositeSpec, UnboundedContinuousTensorSpec\n",
    "from torchrl.envs import (\n",
    "    EnvBase,\n",
    "    Transform,\n",
    "    Compose,\n",
    "    ToTensorImage,\n",
    "    Resize,\n",
    "    TransformedEnv,\n",
    "    UnsqueezeTransform,\n",
    ")\n",
    "\n",
    "from torchrl.envs.transforms.transforms import _apply_to_composite\n",
    "from torchrl.envs.utils import check_env_specs, step_mdp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment\n",
    "#### `_step()`\n",
    "\n",
    "- Read the input keys (such as \"action\") and execute the simulation based on these;\n",
    "- Retrieve observations, done state and reward;\n",
    "- Write the set of observation values along with the reward and done state at the corresponding entries in a new TensorDict.\n",
    "- Merge the output TensorDict (as \"next\" key) in the input TensorDict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "data_path = '/my_bot_v2/src/my_bot_controller/resource/24_10_19_sensorDump/'\n",
    "image_path = 'egoCam/*.png'\n",
    "\n",
    "image_files = glob.glob(data_path + image_path)\n",
    "trajectory_data = pd.read_csv(data_path + 'trajectory.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _step(tensordict):\n",
    "    \n",
    "    # take a step\n",
    "    step = tensordict[\"stepInt\"].item()\n",
    "\n",
    "    image = cv.imread(image_files[step])\n",
    "    # height, width, channels = 480, 640, 3\n",
    "    # image = np.zeros((height, width, channels), dtype=np.uint8)\n",
    "\n",
    "    image_tensor = torch.tensor(image, dtype=torch.float32)\n",
    "\n",
    "    step += 1\n",
    "\n",
    "    step_tensor = torch.tensor(step, dtype=torch.int32)\n",
    "\n",
    "    # laser_readings = \"\"\n",
    "    # laser_tensor = torch.tensor(laser_readings, dtype=torch.float32)\n",
    "\n",
    "    progress = torch.tensor(0, dtype=torch.float32)\n",
    "\n",
    "    reward = progress.view(*tensordict.shape, 1)\n",
    "    done = torch.zeros_like(reward, dtype=torch.bool)\n",
    "\n",
    "\n",
    "    # write observations\n",
    "    # merge the output with input tensordict\n",
    "    out = TensorDict(\n",
    "        {\n",
    "            \"stepInt\": step_tensor,\n",
    "            \"image\": image_tensor,\n",
    "            # \"laser\": laser_tensor,\n",
    "            \"params\": tensordict[\"params\"],\n",
    "            \"reward\": reward,\n",
    "            \"done\": done,\n",
    "        },\n",
    "        tensordict.shape,\n",
    "    )\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### _reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# retrieve observations (observations, reward, done)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _reset(self, tensordict):\n",
    "    if tensordict is None or tensordict.is_empty():\n",
    "        # if no ``tensordict`` is passed, we generate a single set of hyperparameters\n",
    "        # Otherwise, we assume that the input ``tensordict`` contains all the relevant\n",
    "        # parameters to get started.\n",
    "        tensordict = self.gen_params(batch_size=self.batch_size)\n",
    "\n",
    "\n",
    "    # retrieve observations (observations, reward, done)\n",
    "    step = 0\n",
    "\n",
    "    step_tensor = torch.tensor(step, dtype=torch.int32)\n",
    "    # take a step\n",
    "    image = cv.imread(image_files[step])\n",
    "\n",
    "    # height, width, channels = 480, 640, 3\n",
    "    # image = np.zeros((height, width, channels), dtype=np.uint8)\n",
    "    \n",
    "    image_tensor = torch.tensor(image, dtype=torch.float32)\n",
    "\n",
    "    out = TensorDict(\n",
    "        {   \n",
    "            \"stepInt\": step_tensor,\n",
    "            \"image\": image_tensor,\n",
    "            \"params\": tensordict[\"params\"],\n",
    "        },\n",
    "        batch_size=tensordict.shape,\n",
    "    )\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Environment metadata: `env.*_spec`\n",
    "\n",
    "The specs define the input and output domain of the environment. They can also be used to instantiate lazily defined neural networks and test scripts. There are four specs that we must code in our environment:\n",
    "\n",
    "- `EnvBase.observation_spec`: This will be a `CompositeSpec` instance where each key is an observation (a CompositeSpec can be viewed as a dictionary of specs).\n",
    "- `EnvBase.action_spec`: It can be any type of spec, it corresponds to the \"action\" entry in the input tensordict;\n",
    "- `EnvBase.reward_spec`: provides information about the reward space;\n",
    "- `EnvBase.done_spec`: provides information about the space of the done flag.\n",
    "\n",
    "TorchRL specs are organized in two general containers:\n",
    "- input_spec which contains the specs of the information that the step function reads (divided between action_spec containing the action and state_spec containing all the rest),\n",
    "- output_spec which encodes the specs that the step outputs (observation_spec, reward_spec and done_spec).\n",
    "\n",
    "In general, you should not interact directly with output_spec and input_spec but only with their content: observation_spec, reward_spec, done_spec, action_spec and state_spec. TorchRL offers multiple TensorSpec subclasses to encode the environment’s input and output characteristics.\n",
    "\n",
    "##### Specs shape\n",
    "The environment specs leading dimensions must match the environment batch-size. This is done to enforce that every component of an environment (including its transforms) have an accurate representation of the expected input and output shapes. This is something that should be accurately coded in stateful settings. For non batch-locked environments, such as the one in our example (see below), this is irrelevant as the environment batch size will most likely be empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_spec(self, td_params):\n",
    "    # Under the hood, this will populate self.output_spec[\"observation\"]\n",
    "    self.observation_spec = CompositeSpec(\n",
    "        stepInt=BoundedTensorSpec(\n",
    "            low=td_params[\"params\", \"step_start\"],\n",
    "            high=td_params[\"params\", \"step_end\"],\n",
    "            shape=(),\n",
    "            dtype=torch.int32,\n",
    "        ),\n",
    "        image=UnboundedContinuousTensorSpec(\n",
    "            shape=(td_params[\"params\", \"imageHeight\"], td_params[\"params\", \"imageWidth\"], 3),\n",
    "            dtype=torch.float32,\n",
    "        ),\n",
    "        # we need to add the ``params`` to the observation specs, as we want\n",
    "        # to pass it at each step during a rollout\n",
    "        params=make_composite_from_td(td_params[\"params\"]),\n",
    "        shape=(),\n",
    "    )\n",
    "\n",
    "    # action-spec will be automatically wrapped in input_spec when\n",
    "    # `self.action_spec = spec` will be called supported\n",
    "    self.action_spec = CompositeSpec(\n",
    "        action=CompositeSpec(\n",
    "            linear_velocity=BoundedTensorSpec(\n",
    "                low=-td_params[\"params\", \"max_linear_velocity\"],\n",
    "                high=td_params[\"params\", \"max_linear_velocity\"],\n",
    "                shape=(),\n",
    "                dtype=torch.float32,\n",
    "            ),\n",
    "            angular_velocity=BoundedTensorSpec(\n",
    "                low=-td_params[\"params\", \"max_angular_velocity\"],\n",
    "                high=td_params[\"params\", \"max_angular_velocity\"],\n",
    "                shape=(),\n",
    "                dtype=torch.float32,\n",
    "            ),\n",
    "            shape=(),\n",
    "        ),\n",
    "        shape=(),\n",
    "    )\n",
    "    self.reward_spec = UnboundedContinuousTensorSpec(shape=(*td_params.shape, 1))\n",
    "\n",
    "\n",
    "def make_composite_from_td(td):\n",
    "    # custom function to convert a ``tensordict`` in a similar spec structure\n",
    "    # of unbounded values.\n",
    "    composite = CompositeSpec(\n",
    "        {\n",
    "            key: make_composite_from_td(tensor)\n",
    "            if isinstance(tensor, TensorDictBase)\n",
    "            else UnboundedContinuousTensorSpec(\n",
    "                dtype=tensor.dtype, device=tensor.device, shape=tensor.shape\n",
    "            )\n",
    "            for key, tensor in td.items()\n",
    "        },\n",
    "        shape=td.shape,\n",
    "    )\n",
    "    return composite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _set_seed(self, seed: Optional[int]):\n",
    "    rng = torch.manual_seed(seed)\n",
    "    self.rng = rng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_params(batch_size=None) -> TensorDictBase:\n",
    "    \"\"\"Returns a ``tensordict`` containing the physical parameters such as gravitational force and torque or speed limits.\"\"\"\n",
    "    if batch_size is None:\n",
    "        batch_size = []\n",
    "    td = TensorDict(\n",
    "        {\n",
    "            \"params\": TensorDict(\n",
    "                {\n",
    "                    \"step_start\": 0,\n",
    "                    \"step_end\": 20,\n",
    "                    \"max_linear_velocity\": 1.0,\n",
    "                    \"max_angular_velocity\": 1.0,\n",
    "                    \"imageHeight\": 480,\n",
    "                    \"imageWidth\": 640,\n",
    "                },\n",
    "                [],\n",
    "            )\n",
    "        },\n",
    "        [],\n",
    "    )\n",
    "    if batch_size:\n",
    "        td = td.expand(batch_size).contiguous()\n",
    "    return td"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Environment class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_botEnv(EnvBase):\n",
    "    metadata = {\n",
    "        \"render_modes\": [\"human\", \"rgb_array\"],\n",
    "        \"render_fps\": 30,\n",
    "    }\n",
    "    batch_locked = True\n",
    "\n",
    "    def __init__(self, td_params=None, seed=None, device=\"cpu\"):\n",
    "        if td_params is None:\n",
    "            td_params = self.gen_params()\n",
    "\n",
    "        super().__init__(device=device, batch_size=[])\n",
    "        self._make_spec(td_params)\n",
    "        if seed is None:\n",
    "            seed = torch.empty((), dtype=torch.int64).random_().item()\n",
    "        self.set_seed(seed)\n",
    "\n",
    "    # Helpers: _make_step and gen_params\n",
    "    gen_params = staticmethod(gen_params)\n",
    "    _make_spec = _make_spec\n",
    "\n",
    "    # Mandatory methods: _step, _reset and _set_seed\n",
    "    _reset = _reset\n",
    "    _step = staticmethod(_step)\n",
    "    _set_seed = _set_seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check environment implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-26 04:39:43,602 [torchrl][INFO] check_env_specs succeeded!\n"
     ]
    }
   ],
   "source": [
    "env = my_botEnv()\n",
    "check_env_specs(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation_spec: CompositeSpec(\n",
      "    stepInt: BoundedTensorSpec(\n",
      "        shape=torch.Size([]),\n",
      "        space=ContinuousBox(\n",
      "            low=Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int32, contiguous=True),\n",
      "            high=Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int32, contiguous=True)),\n",
      "        device=cpu,\n",
      "        dtype=torch.int32,\n",
      "        domain=continuous),\n",
      "    image: UnboundedContinuousTensorSpec(\n",
      "        shape=torch.Size([480, 640, 3]),\n",
      "        space=None,\n",
      "        device=cpu,\n",
      "        dtype=torch.float32,\n",
      "        domain=continuous),\n",
      "    params: CompositeSpec(\n",
      "        step_start: UnboundedContinuousTensorSpec(\n",
      "            shape=torch.Size([]),\n",
      "            space=None,\n",
      "            device=cpu,\n",
      "            dtype=torch.int64,\n",
      "            domain=discrete),\n",
      "        step_end: UnboundedContinuousTensorSpec(\n",
      "            shape=torch.Size([]),\n",
      "            space=None,\n",
      "            device=cpu,\n",
      "            dtype=torch.int64,\n",
      "            domain=discrete),\n",
      "        max_linear_velocity: UnboundedContinuousTensorSpec(\n",
      "            shape=torch.Size([]),\n",
      "            space=None,\n",
      "            device=cpu,\n",
      "            dtype=torch.float32,\n",
      "            domain=continuous),\n",
      "        max_angular_velocity: UnboundedContinuousTensorSpec(\n",
      "            shape=torch.Size([]),\n",
      "            space=None,\n",
      "            device=cpu,\n",
      "            dtype=torch.float32,\n",
      "            domain=continuous),\n",
      "        imageHeight: UnboundedContinuousTensorSpec(\n",
      "            shape=torch.Size([]),\n",
      "            space=None,\n",
      "            device=cpu,\n",
      "            dtype=torch.int64,\n",
      "            domain=discrete),\n",
      "        imageWidth: UnboundedContinuousTensorSpec(\n",
      "            shape=torch.Size([]),\n",
      "            space=None,\n",
      "            device=cpu,\n",
      "            dtype=torch.int64,\n",
      "            domain=discrete),\n",
      "        device=cpu,\n",
      "        shape=torch.Size([])),\n",
      "    device=cpu,\n",
      "    shape=torch.Size([]))\n",
      "state_spec: CompositeSpec(\n",
      ",\n",
      "    device=cpu,\n",
      "    shape=torch.Size([]))\n",
      "reward_spec: UnboundedContinuousTensorSpec(\n",
      "    shape=torch.Size([1]),\n",
      "    space=ContinuousBox(\n",
      "        low=Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, contiguous=True),\n",
      "        high=Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, contiguous=True)),\n",
      "    device=cpu,\n",
      "    dtype=torch.float32,\n",
      "    domain=continuous)\n"
     ]
    }
   ],
   "source": [
    "print(\"observation_spec:\", env.observation_spec)\n",
    "print(\"state_spec:\", env.state_spec)\n",
    "print(\"reward_spec:\", env.reward_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reset tensordict TensorDict(\n",
      "    fields={\n",
      "        done: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        image: Tensor(shape=torch.Size([480, 640, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        params: TensorDict(\n",
      "            fields={\n",
      "                imageHeight: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "                imageWidth: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "                max_angular_velocity: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                max_linear_velocity: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                step_end: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "                step_start: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, is_shared=False)},\n",
      "            batch_size=torch.Size([]),\n",
      "            device=None,\n",
      "            is_shared=False),\n",
      "        stepInt: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int32, is_shared=False),\n",
      "        terminated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "    batch_size=torch.Size([]),\n",
      "    device=None,\n",
      "    is_shared=False)\n"
     ]
    }
   ],
   "source": [
    "td = env.reset()\n",
    "print(\"reset tensordict\", td)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random step tensordict TensorDict(\n",
      "    fields={\n",
      "        action: TensorDict(\n",
      "            fields={\n",
      "                angular_velocity: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                linear_velocity: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
      "            batch_size=torch.Size([]),\n",
      "            device=cpu,\n",
      "            is_shared=False),\n",
      "        done: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        image: Tensor(shape=torch.Size([480, 640, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        next: TensorDict(\n",
      "            fields={\n",
      "                done: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                image: Tensor(shape=torch.Size([480, 640, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                params: TensorDict(\n",
      "                    fields={\n",
      "                        imageHeight: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "                        imageWidth: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "                        max_angular_velocity: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                        max_linear_velocity: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                        step_end: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "                        step_start: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, is_shared=False)},\n",
      "                    batch_size=torch.Size([]),\n",
      "                    device=None,\n",
      "                    is_shared=False),\n",
      "                reward: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                stepInt: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int32, is_shared=False),\n",
      "                terminated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "            batch_size=torch.Size([]),\n",
      "            device=None,\n",
      "            is_shared=False),\n",
      "        params: TensorDict(\n",
      "            fields={\n",
      "                imageHeight: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "                imageWidth: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "                max_angular_velocity: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                max_linear_velocity: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                step_end: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "                step_start: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, is_shared=False)},\n",
      "            batch_size=torch.Size([]),\n",
      "            device=None,\n",
      "            is_shared=False),\n",
      "        stepInt: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int32, is_shared=False),\n",
      "        terminated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "    batch_size=torch.Size([]),\n",
      "    device=None,\n",
      "    is_shared=False)\n"
     ]
    }
   ],
   "source": [
    "td = env.rand_step(td)\n",
    "print(\"random step tensordict\", td)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_env = my_botEnv()\n",
    "transform = Compose(ToTensorImage(in_keys=[\"image\"]), Resize(64, 64, in_keys=[\"image\"]))\n",
    "env = TransformedEnv(base_env, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-25 00:21:58,327 [torchrl][INFO] check_env_specs succeeded!\n"
     ]
    }
   ],
   "source": [
    "check_env_specs(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data from rollout: TensorDict(\n",
      "    fields={\n",
      "        action: TensorDict(\n",
      "            fields={\n",
      "                action: TensorDict(\n",
      "                    fields={\n",
      "                        angular_velocity: Tensor(shape=torch.Size([15]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                        linear_velocity: Tensor(shape=torch.Size([15]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
      "                    batch_size=torch.Size([15]),\n",
      "                    device=cpu,\n",
      "                    is_shared=False)},\n",
      "            batch_size=torch.Size([15]),\n",
      "            device=cpu,\n",
      "            is_shared=False),\n",
      "        done: Tensor(shape=torch.Size([15, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        image: Tensor(shape=torch.Size([15, 3, 64, 64]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        next: TensorDict(\n",
      "            fields={\n",
      "                done: Tensor(shape=torch.Size([15, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                image: Tensor(shape=torch.Size([15, 3, 64, 64]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                params: TensorDict(\n",
      "                    fields={\n",
      "                        imageHeight: Tensor(shape=torch.Size([15]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "                        imageWidth: Tensor(shape=torch.Size([15]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "                        max_angular_velocity: Tensor(shape=torch.Size([15]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                        max_linear_velocity: Tensor(shape=torch.Size([15]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                        step_end: Tensor(shape=torch.Size([15]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "                        step_start: Tensor(shape=torch.Size([15]), device=cpu, dtype=torch.int64, is_shared=False)},\n",
      "                    batch_size=torch.Size([15]),\n",
      "                    device=None,\n",
      "                    is_shared=False),\n",
      "                reward: Tensor(shape=torch.Size([15, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                stepInt: Tensor(shape=torch.Size([15]), device=cpu, dtype=torch.int32, is_shared=False),\n",
      "                terminated: Tensor(shape=torch.Size([15, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "            batch_size=torch.Size([15]),\n",
      "            device=None,\n",
      "            is_shared=False),\n",
      "        params: TensorDict(\n",
      "            fields={\n",
      "                imageHeight: Tensor(shape=torch.Size([15]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "                imageWidth: Tensor(shape=torch.Size([15]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "                max_angular_velocity: Tensor(shape=torch.Size([15]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                max_linear_velocity: Tensor(shape=torch.Size([15]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                step_end: Tensor(shape=torch.Size([15]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "                step_start: Tensor(shape=torch.Size([15]), device=cpu, dtype=torch.int64, is_shared=False)},\n",
      "            batch_size=torch.Size([15]),\n",
      "            device=None,\n",
      "            is_shared=False),\n",
      "        stepInt: Tensor(shape=torch.Size([15]), device=cpu, dtype=torch.int32, is_shared=False),\n",
      "        terminated: Tensor(shape=torch.Size([15, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "    batch_size=torch.Size([15]),\n",
      "    device=None,\n",
      "    is_shared=False)\n"
     ]
    }
   ],
   "source": [
    "def simple_rollout(steps=15):\n",
    "    # preallocate:\n",
    "    data = TensorDict({}, [steps])\n",
    "    # reset\n",
    "    _data = env.reset()\n",
    "    for i in range(steps):\n",
    "        _data[\"action\"] = env.action_spec.rand()\n",
    "        _data = env.step(_data)\n",
    "        data[i] = _data\n",
    "        _data = step_mdp(_data, keep_other=True)\n",
    "    return data\n",
    "\n",
    "\n",
    "print(\"data from rollout:\", simple_rollout(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected a tensordict with shape==env.batch_size, got torch.Size([1]) and torch.Size([])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;66;03m# number of environments to be executed in batch\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m td \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreset (batch size of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m, td)\n\u001b[1;32m      4\u001b[0m td \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mrand_step(td)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchrl/envs/common.py:2163\u001b[0m, in \u001b[0;36mEnvBase.reset\u001b[0;34m(self, tensordict, **kwargs)\u001b[0m\n\u001b[1;32m   2148\u001b[0m \u001b[38;5;124;03m\"\"\"Resets the environment.\u001b[39;00m\n\u001b[1;32m   2149\u001b[0m \n\u001b[1;32m   2150\u001b[0m \u001b[38;5;124;03mAs for step and _step, only the private method :obj:`_reset` should be overwritten by EnvBase subclasses.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2160\u001b[0m \n\u001b[1;32m   2161\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2162\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tensordict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2163\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_assert_tensordict_shape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensordict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2165\u001b[0m tensordict_reset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset(tensordict, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   2166\u001b[0m \u001b[38;5;66;03m# We assume that this is done properly\u001b[39;00m\n\u001b[1;32m   2167\u001b[0m \u001b[38;5;66;03m# if reset.device != self.device:\u001b[39;00m\n\u001b[1;32m   2168\u001b[0m \u001b[38;5;66;03m#     reset = reset.to(self.device, non_blocking=True)\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchrl/envs/common.py:2271\u001b[0m, in \u001b[0;36mEnvBase._assert_tensordict_shape\u001b[0;34m(self, tensordict)\u001b[0m\n\u001b[1;32m   2267\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_assert_tensordict_shape\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensordict: TensorDictBase) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2268\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2269\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_locked \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size \u001b[38;5;241m!=\u001b[39m ()\n\u001b[1;32m   2270\u001b[0m     ) \u001b[38;5;129;01mand\u001b[39;00m tensordict\u001b[38;5;241m.\u001b[39mbatch_size \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size:\n\u001b[0;32m-> 2271\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   2272\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected a tensordict with shape==env.batch_size, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2273\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtensordict\u001b[38;5;241m.\u001b[39mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2274\u001b[0m         )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected a tensordict with shape==env.batch_size, got torch.Size([1]) and torch.Size([])"
     ]
    }
   ],
   "source": [
    "batch_size = 1 # number of environments to be executed in batch\n",
    "td = env.reset(env.gen_params(batch_size=[batch_size]))\n",
    "print(f\"reset (batch size of {batch_size})\", td)\n",
    "td = env.rand_step(td)\n",
    "print(f\"rand step (batch size of {batch_size})\", td)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a Simple Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "env.set_seed(0)\n",
    "\n",
    "net = nn.Sequential(\n",
    "    nn.LazyLinear(64),\n",
    "    nn.Tanh(),\n",
    "    nn.LazyLinear(64),\n",
    "    nn.Tanh(),\n",
    "    nn.LazyLinear(64),\n",
    "    nn.Tanh(),\n",
    "    nn.LazyLinear(1),\n",
    ")\n",
    "policy = TensorDictModule(\n",
    "    net,\n",
    "    in_keys=[\"observation\"],\n",
    "    out_keys=[\"action\"],\n",
    ")\n",
    "\n",
    "optim = torch.optim.Adam(policy.parameters(), lr=2e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training loop\n",
    "We will successively:\n",
    "\n",
    "- generate a trajectory\n",
    "- sum the rewards\n",
    "- backpropagate through the graph defined by these operations\n",
    "- clip the gradient norm and make an optimization step\n",
    "- repeat\n",
    "\n",
    "At the end of the training loop, we should have a final reward close to 0 which demonstrates that the pendulum is upward and still as desired.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "pbar = tqdm.tqdm(range(20_000 // batch_size))\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, 20_000)\n",
    "logs = defaultdict(list)\n",
    "\n",
    "for _ in pbar:\n",
    "    init_td = env.reset(env.gen_params(batch_size=[batch_size]))\n",
    "    rollout = env.rollout(100, policy, tensordict=init_td, auto_reset=False)\n",
    "    traj_return = rollout[\"next\", \"reward\"].mean()\n",
    "    (-traj_return).backward()\n",
    "    gn = torch.nn.utils.clip_grad_norm_(net.parameters(), 1.0)\n",
    "    optim.step()\n",
    "    optim.zero_grad()\n",
    "    pbar.set_description(\n",
    "        f\"reward: {traj_return: 4.4f}, \"\n",
    "        f\"last reward: {rollout[..., -1]['next', 'reward'].mean(): 4.4f}, gradient norm: {gn: 4.4}\"\n",
    "    )\n",
    "    logs[\"return\"].append(traj_return.item())\n",
    "    logs[\"last_reward\"].append(rollout[..., -1][\"next\", \"reward\"].mean().item())\n",
    "    scheduler.step()\n",
    "\n",
    "\n",
    "def plot():\n",
    "    import matplotlib\n",
    "    from matplotlib import pyplot as plt\n",
    "\n",
    "    is_ipython = \"inline\" in matplotlib.get_backend()\n",
    "    if is_ipython:\n",
    "        from IPython import display\n",
    "\n",
    "    with plt.ion():\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(logs[\"return\"])\n",
    "        plt.title(\"returns\")\n",
    "        plt.xlabel(\"iteration\")\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(logs[\"last_reward\"])\n",
    "        plt.title(\"last reward\")\n",
    "        plt.xlabel(\"iteration\")\n",
    "        if is_ipython:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Troubleshoot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _step(tensordict):\n",
    "    # take a step\n",
    "    step = tensordict[\"stepInt\"].item()  # Can use .item() since we're guaranteed single instance\n",
    "    \n",
    "    # Create single image\n",
    "    image = cv.imread(image_files[step])\n",
    "    image_tensor = torch.tensor(image, dtype=torch.float32)\n",
    "\n",
    "    step += 1\n",
    "    step_tensor = torch.tensor(step, dtype=torch.int32)\n",
    "\n",
    "    progress = torch.tensor(0, dtype=torch.float32)\n",
    "    reward = progress.view(1)  # Single reward\n",
    "    done = torch.zeros_like(reward, dtype=torch.bool)\n",
    "\n",
    "    # Create TensorDict without explicit shape argument\n",
    "    out = TensorDict(\n",
    "        {\n",
    "            \"stepInt\": step_tensor,\n",
    "            \"image\": image_tensor,\n",
    "            \"params\": tensordict[\"params\"],\n",
    "            \"reward\": reward,\n",
    "            \"done\": done,\n",
    "        },\n",
    "        batch_size=[],  # Use batch_size instead of shape\n",
    "    )\n",
    "\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _reset(self, tensordict):\n",
    "    if tensordict is None or tensordict.is_empty():\n",
    "        # if no ``tensordict`` is passed, we generate a single set of hyperparameters\n",
    "        # Otherwise, we assume that the input ``tensordict`` contains all the relevant\n",
    "        # parameters to get started.\n",
    "        tensordict = self.gen_params(batch_size=self.batch_size)\n",
    "\n",
    "\n",
    "    # retrieve observations (observations, reward, done)\n",
    "    step = 0\n",
    "\n",
    "    step_tensor = torch.tensor(step, dtype=torch.int32)\n",
    "    # take a step\n",
    "    # image = cv.imread(image_files[step])\n",
    "\n",
    "    image = cv.imread(image_files[step])\n",
    "    \n",
    "    image_tensor = torch.tensor(image, dtype=torch.float32)\n",
    "\n",
    "    out = TensorDict(\n",
    "        {   \n",
    "            \"stepInt\": step_tensor,\n",
    "            \"image\": image_tensor,\n",
    "            \"params\": tensordict[\"params\"],\n",
    "        },\n",
    "        batch_size=tensordict.shape,\n",
    "    )\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _set_seed(self, seed: Optional[int]):\n",
    "    rng = torch.manual_seed(seed)\n",
    "    self.rng = rng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_spec(self, td_params):\n",
    "    self.observation_spec = CompositeSpec(\n",
    "        stepInt=BoundedTensorSpec(\n",
    "            low=td_params[\"params\", \"step_start\"],\n",
    "            high=td_params[\"params\", \"step_end\"],\n",
    "            shape=(),  # Single instance, no batch dimension\n",
    "            dtype=torch.int32,\n",
    "        ),\n",
    "        image=UnboundedContinuousTensorSpec(\n",
    "            shape=(td_params[\"params\", \"imageHeight\"], \n",
    "                  td_params[\"params\", \"imageWidth\"], 3),  # Single image shape\n",
    "            dtype=torch.float32,\n",
    "        ),\n",
    "        params=make_composite_from_td(td_params[\"params\"]),\n",
    "        shape=(),  # Single instance\n",
    "    )\n",
    "\n",
    "    self.action_spec = CompositeSpec(\n",
    "        action=CompositeSpec(\n",
    "            linear_velocity=BoundedTensorSpec(\n",
    "                low=-td_params[\"params\", \"max_linear_velocity\"],\n",
    "                high=td_params[\"params\", \"max_linear_velocity\"],\n",
    "                shape=(),  # Single instance\n",
    "                dtype=torch.float32,\n",
    "            ),\n",
    "            angular_velocity=BoundedTensorSpec(\n",
    "                low=-td_params[\"params\", \"max_angular_velocity\"],\n",
    "                high=td_params[\"params\", \"max_angular_velocity\"],\n",
    "                shape=(),  # Single instance\n",
    "                dtype=torch.float32,\n",
    "            ),\n",
    "            shape=(),\n",
    "        ),\n",
    "        shape=(),\n",
    "    )\n",
    "    self.reward_spec = UnboundedContinuousTensorSpec(shape=(1,))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_params(*args, **kwargs) -> TensorDictBase:\n",
    "    \"\"\"Returns a tensordict containing the parameters. Ignores any arguments since we're batch_locked.\"\"\"\n",
    "    td = TensorDict(\n",
    "        {\n",
    "            \"params\": TensorDict(\n",
    "                {\n",
    "                    \"step_start\": 0,\n",
    "                    \"step_end\": 20,\n",
    "                    \"max_linear_velocity\": 1.0,\n",
    "                    \"max_angular_velocity\": 1.0,\n",
    "                    \"imageHeight\": 480,\n",
    "                    \"imageWidth\": 640,\n",
    "                },\n",
    "                batch_size=[],\n",
    "            )\n",
    "        },\n",
    "        batch_size=[],\n",
    "    )\n",
    "    return td\n",
    "\n",
    "class my_botEnv(EnvBase):\n",
    "    metadata = {\n",
    "        \"render_modes\": [\"human\", \"rgb_array\"],\n",
    "        \"render_fps\": 30,\n",
    "    }\n",
    "    batch_locked = True  # Lock to single instance since we're using Gazebo\n",
    "\n",
    "    def __init__(self, td_params=None, seed=None, device=\"cpu\"):\n",
    "        if td_params is None:\n",
    "            td_params = self.gen_params()\n",
    "\n",
    "        super().__init__(device=device, batch_size=[])\n",
    "        self._make_spec(td_params)\n",
    "        if seed is None:\n",
    "            seed = torch.empty((), dtype=torch.int64).random_().item()\n",
    "        self.set_seed(seed)\n",
    "\n",
    "    # Helpers: _make_step and gen_params\n",
    "    gen_params = staticmethod(gen_params)\n",
    "    _make_spec = _make_spec\n",
    "\n",
    "    # Mandatory methods: _step, _reset and _set_seed\n",
    "    _reset = _reset\n",
    "    _step = staticmethod(_step)\n",
    "    _set_seed = _set_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-26 04:46:03,933 [torchrl][INFO] check_env_specs succeeded!\n"
     ]
    }
   ],
   "source": [
    "env = my_botEnv()\n",
    "check_env_specs(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation_spec: Composite(\n",
      "    stepInt: BoundedDiscrete(\n",
      "        shape=torch.Size([]),\n",
      "        space=ContinuousBox(\n",
      "            low=Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int32, contiguous=True),\n",
      "            high=Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int32, contiguous=True)),\n",
      "        device=cpu,\n",
      "        dtype=torch.int32,\n",
      "        domain=discrete),\n",
      "    image: UnboundedContinuous(\n",
      "        shape=torch.Size([480, 640, 3]),\n",
      "        space=ContinuousBox(\n",
      "            low=Tensor(shape=torch.Size([480, 640, 3]), device=cpu, dtype=torch.float32, contiguous=True),\n",
      "            high=Tensor(shape=torch.Size([480, 640, 3]), device=cpu, dtype=torch.float32, contiguous=True)),\n",
      "        device=cpu,\n",
      "        dtype=torch.float32,\n",
      "        domain=continuous),\n",
      "    params: Composite(\n",
      "        step_start: UnboundedDiscrete(\n",
      "            shape=torch.Size([]),\n",
      "            space=ContinuousBox(\n",
      "                low=Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, contiguous=True),\n",
      "                high=Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, contiguous=True)),\n",
      "            device=cpu,\n",
      "            dtype=torch.int64,\n",
      "            domain=discrete),\n",
      "        step_end: UnboundedDiscrete(\n",
      "            shape=torch.Size([]),\n",
      "            space=ContinuousBox(\n",
      "                low=Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, contiguous=True),\n",
      "                high=Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, contiguous=True)),\n",
      "            device=cpu,\n",
      "            dtype=torch.int64,\n",
      "            domain=discrete),\n",
      "        max_linear_velocity: UnboundedContinuous(\n",
      "            shape=torch.Size([]),\n",
      "            space=ContinuousBox(\n",
      "                low=Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, contiguous=True),\n",
      "                high=Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, contiguous=True)),\n",
      "            device=cpu,\n",
      "            dtype=torch.float32,\n",
      "            domain=continuous),\n",
      "        max_angular_velocity: UnboundedContinuous(\n",
      "            shape=torch.Size([]),\n",
      "            space=ContinuousBox(\n",
      "                low=Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, contiguous=True),\n",
      "                high=Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, contiguous=True)),\n",
      "            device=cpu,\n",
      "            dtype=torch.float32,\n",
      "            domain=continuous),\n",
      "        imageHeight: UnboundedDiscrete(\n",
      "            shape=torch.Size([]),\n",
      "            space=ContinuousBox(\n",
      "                low=Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, contiguous=True),\n",
      "                high=Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, contiguous=True)),\n",
      "            device=cpu,\n",
      "            dtype=torch.int64,\n",
      "            domain=discrete),\n",
      "        imageWidth: UnboundedDiscrete(\n",
      "            shape=torch.Size([]),\n",
      "            space=ContinuousBox(\n",
      "                low=Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, contiguous=True),\n",
      "                high=Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, contiguous=True)),\n",
      "            device=cpu,\n",
      "            dtype=torch.int64,\n",
      "            domain=discrete),\n",
      "        device=cpu,\n",
      "        shape=torch.Size([])),\n",
      "    device=cpu,\n",
      "    shape=torch.Size([]))\n",
      "state_spec: Composite(\n",
      ",\n",
      "    device=cpu,\n",
      "    shape=torch.Size([]))\n",
      "reward_spec: UnboundedContinuous(\n",
      "    shape=torch.Size([1]),\n",
      "    space=ContinuousBox(\n",
      "        low=Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, contiguous=True),\n",
      "        high=Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, contiguous=True)),\n",
      "    device=cpu,\n",
      "    dtype=torch.float32,\n",
      "    domain=continuous)\n"
     ]
    }
   ],
   "source": [
    "print(\"observation_spec:\", env.observation_spec)\n",
    "print(\"state_spec:\", env.state_spec)\n",
    "print(\"reward_spec:\", env.reward_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reset tensordict TensorDict(\n",
      "    fields={\n",
      "        done: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        image: Tensor(shape=torch.Size([480, 640, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        params: TensorDict(\n",
      "            fields={\n",
      "                imageHeight: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "                imageWidth: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "                max_angular_velocity: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                max_linear_velocity: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                step_end: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "                step_start: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, is_shared=False)},\n",
      "            batch_size=torch.Size([]),\n",
      "            device=None,\n",
      "            is_shared=False),\n",
      "        stepInt: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int32, is_shared=False),\n",
      "        terminated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "    batch_size=torch.Size([]),\n",
      "    device=None,\n",
      "    is_shared=False)\n"
     ]
    }
   ],
   "source": [
    "td = env.reset()\n",
    "print(\"reset tensordict\", td)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random step tensordict TensorDict(\n",
      "    fields={\n",
      "        action: TensorDict(\n",
      "            fields={\n",
      "                angular_velocity: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                linear_velocity: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
      "            batch_size=torch.Size([]),\n",
      "            device=cpu,\n",
      "            is_shared=False),\n",
      "        done: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        image: Tensor(shape=torch.Size([480, 640, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        next: TensorDict(\n",
      "            fields={\n",
      "                done: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                image: Tensor(shape=torch.Size([480, 640, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                params: TensorDict(\n",
      "                    fields={\n",
      "                        imageHeight: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "                        imageWidth: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "                        max_angular_velocity: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                        max_linear_velocity: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                        step_end: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "                        step_start: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, is_shared=False)},\n",
      "                    batch_size=torch.Size([]),\n",
      "                    device=None,\n",
      "                    is_shared=False),\n",
      "                reward: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                stepInt: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int32, is_shared=False),\n",
      "                terminated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "            batch_size=torch.Size([]),\n",
      "            device=None,\n",
      "            is_shared=False),\n",
      "        params: TensorDict(\n",
      "            fields={\n",
      "                imageHeight: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "                imageWidth: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "                max_angular_velocity: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                max_linear_velocity: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                step_end: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "                step_start: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, is_shared=False)},\n",
      "            batch_size=torch.Size([]),\n",
      "            device=None,\n",
      "            is_shared=False),\n",
      "        stepInt: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int32, is_shared=False),\n",
      "        terminated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "    batch_size=torch.Size([]),\n",
      "    device=None,\n",
      "    is_shared=False)\n"
     ]
    }
   ],
   "source": [
    "td = env.rand_step(td)\n",
    "print(\"random step tensordict\", td)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_env = my_botEnv()\n",
    "transform = Compose(ToTensorImage(in_keys=[\"image\"]), Resize(64, 64, in_keys=[\"image\"]))\n",
    "env = TransformedEnv(base_env, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-26 04:46:18,553 [torchrl][INFO] check_env_specs succeeded!\n"
     ]
    }
   ],
   "source": [
    "check_env_specs(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data from rollout: TensorDict(\n",
      "    fields={\n",
      "        action: TensorDict(\n",
      "            fields={\n",
      "                action: TensorDict(\n",
      "                    fields={\n",
      "                        angular_velocity: Tensor(shape=torch.Size([15]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                        linear_velocity: Tensor(shape=torch.Size([15]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
      "                    batch_size=torch.Size([15]),\n",
      "                    device=cpu,\n",
      "                    is_shared=False)},\n",
      "            batch_size=torch.Size([15]),\n",
      "            device=cpu,\n",
      "            is_shared=False),\n",
      "        done: Tensor(shape=torch.Size([15, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        image: Tensor(shape=torch.Size([15, 3, 64, 64]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        next: TensorDict(\n",
      "            fields={\n",
      "                done: Tensor(shape=torch.Size([15, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                image: Tensor(shape=torch.Size([15, 3, 64, 64]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                params: TensorDict(\n",
      "                    fields={\n",
      "                        imageHeight: Tensor(shape=torch.Size([15]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "                        imageWidth: Tensor(shape=torch.Size([15]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "                        max_angular_velocity: Tensor(shape=torch.Size([15]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                        max_linear_velocity: Tensor(shape=torch.Size([15]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                        step_end: Tensor(shape=torch.Size([15]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "                        step_start: Tensor(shape=torch.Size([15]), device=cpu, dtype=torch.int64, is_shared=False)},\n",
      "                    batch_size=torch.Size([15]),\n",
      "                    device=None,\n",
      "                    is_shared=False),\n",
      "                reward: Tensor(shape=torch.Size([15, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                stepInt: Tensor(shape=torch.Size([15]), device=cpu, dtype=torch.int32, is_shared=False),\n",
      "                terminated: Tensor(shape=torch.Size([15, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "            batch_size=torch.Size([15]),\n",
      "            device=None,\n",
      "            is_shared=False),\n",
      "        params: TensorDict(\n",
      "            fields={\n",
      "                imageHeight: Tensor(shape=torch.Size([15]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "                imageWidth: Tensor(shape=torch.Size([15]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "                max_angular_velocity: Tensor(shape=torch.Size([15]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                max_linear_velocity: Tensor(shape=torch.Size([15]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                step_end: Tensor(shape=torch.Size([15]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "                step_start: Tensor(shape=torch.Size([15]), device=cpu, dtype=torch.int64, is_shared=False)},\n",
      "            batch_size=torch.Size([15]),\n",
      "            device=None,\n",
      "            is_shared=False),\n",
      "        stepInt: Tensor(shape=torch.Size([15]), device=cpu, dtype=torch.int32, is_shared=False),\n",
      "        terminated: Tensor(shape=torch.Size([15, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "    batch_size=torch.Size([15]),\n",
      "    device=None,\n",
      "    is_shared=False)\n"
     ]
    }
   ],
   "source": [
    "def simple_rollout(steps=15):\n",
    "    # preallocate:\n",
    "    data = TensorDict({}, [steps])\n",
    "    # reset\n",
    "    _data = env.reset()\n",
    "    for i in range(steps):\n",
    "        _data[\"action\"] = env.action_spec.rand()\n",
    "        _data = env.step(_data)\n",
    "        data[i] = _data\n",
    "        _data = step_mdp(_data, keep_other=True)\n",
    "    return data\n",
    "\n",
    "\n",
    "print(\"data from rollout:\", simple_rollout(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reset (batch size of 1) TensorDict(\n",
      "    fields={\n",
      "        done: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        image: Tensor(shape=torch.Size([3, 64, 64]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        params: TensorDict(\n",
      "            fields={\n",
      "                imageHeight: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "                imageWidth: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "                max_angular_velocity: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                max_linear_velocity: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                step_end: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "                step_start: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, is_shared=False)},\n",
      "            batch_size=torch.Size([]),\n",
      "            device=None,\n",
      "            is_shared=False),\n",
      "        stepInt: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int32, is_shared=False),\n",
      "        terminated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "    batch_size=torch.Size([]),\n",
      "    device=None,\n",
      "    is_shared=False)\n",
      "rand step (batch size of 1) TensorDict(\n",
      "    fields={\n",
      "        action: TensorDict(\n",
      "            fields={\n",
      "                angular_velocity: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                linear_velocity: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
      "            batch_size=torch.Size([]),\n",
      "            device=cpu,\n",
      "            is_shared=False),\n",
      "        done: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        image: Tensor(shape=torch.Size([3, 64, 64]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        next: TensorDict(\n",
      "            fields={\n",
      "                done: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                image: Tensor(shape=torch.Size([3, 64, 64]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                params: TensorDict(\n",
      "                    fields={\n",
      "                        imageHeight: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "                        imageWidth: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "                        max_angular_velocity: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                        max_linear_velocity: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                        step_end: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "                        step_start: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, is_shared=False)},\n",
      "                    batch_size=torch.Size([]),\n",
      "                    device=None,\n",
      "                    is_shared=False),\n",
      "                reward: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                stepInt: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int32, is_shared=False),\n",
      "                terminated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "            batch_size=torch.Size([]),\n",
      "            device=None,\n",
      "            is_shared=False),\n",
      "        params: TensorDict(\n",
      "            fields={\n",
      "                imageHeight: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "                imageWidth: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "                max_angular_velocity: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                max_linear_velocity: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                step_end: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, is_shared=False),\n",
      "                step_start: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int64, is_shared=False)},\n",
      "            batch_size=torch.Size([]),\n",
      "            device=None,\n",
      "            is_shared=False),\n",
      "        stepInt: Tensor(shape=torch.Size([]), device=cpu, dtype=torch.int32, is_shared=False),\n",
      "        terminated: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "    batch_size=torch.Size([]),\n",
      "    device=None,\n",
      "    is_shared=False)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1 # number of environments to be executed in batch\n",
    "td = env.reset(env.gen_params(batch_size=[batch_size]))\n",
    "print(f\"reset (batch size of {batch_size})\", td)\n",
    "td = env.rand_step(td)\n",
    "print(f\"rand step (batch size of {batch_size})\", td)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a Simple Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "env.set_seed(0)\n",
    "\n",
    "net = nn.Sequential(\n",
    "    nn.LazyLinear(64),\n",
    "    nn.Tanh(),\n",
    "    nn.LazyLinear(64),\n",
    "    nn.Tanh(),\n",
    "    nn.LazyLinear(64),\n",
    "    nn.Tanh(),\n",
    "    nn.LazyLinear(1),\n",
    ")\n",
    "policy = TensorDictModule(\n",
    "    net,\n",
    "    in_keys=[\"observation\"],\n",
    "    out_keys=[\"action\"],\n",
    ")\n",
    "\n",
    "optim = torch.optim.Adam(policy.parameters(), lr=2e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training loop\n",
    "We will successively:\n",
    "\n",
    "- generate a trajectory\n",
    "- sum the rewards\n",
    "- backpropagate through the graph defined by these operations\n",
    "- clip the gradient norm and make an optimization step\n",
    "- repeat\n",
    "\n",
    "At the end of the training loop, we should have a final reward close to 0 which demonstrates that the pendulum is upward and still as desired.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"Some tensors that are necessary for the module call may not have not been found in the input tensordict: the following inputs are None: {'observation'}.\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;31mRuntimeError\u001b[0m: TensorDictModule failed with operation\n    Sequential(\n      (0): LazyLinear(in_features=0, out_features=64, bias=True)\n      (1): Tanh()\n      (2): LazyLinear(in_features=0, out_features=64, bias=True)\n      (3): Tanh()\n      (4): LazyLinear(in_features=0, out_features=64, bias=True)\n      (5): Tanh()\n      (6): LazyLinear(in_features=0, out_features=1, bias=True)\n    )\n    in_keys=['observation']\n    out_keys=['action'].",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m pbar:\n\u001b[1;32m      7\u001b[0m     init_td \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset(env\u001b[38;5;241m.\u001b[39mgen_params(batch_size\u001b[38;5;241m=\u001b[39m[batch_size]))\n\u001b[0;32m----> 8\u001b[0m     rollout \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensordict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_td\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauto_reset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     traj_return \u001b[38;5;241m=\u001b[39m rollout[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnext\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreward\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m     10\u001b[0m     (\u001b[38;5;241m-\u001b[39mtraj_return)\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchrl/envs/common.py:2635\u001b[0m, in \u001b[0;36mEnvBase.rollout\u001b[0;34m(self, max_steps, policy, callback, auto_reset, auto_cast_to_device, break_when_any_done, break_when_all_done, return_contiguous, tensordict, set_truncated, out, trust_policy)\u001b[0m\n\u001b[1;32m   2625\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   2626\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensordict\u001b[39m\u001b[38;5;124m\"\u001b[39m: tensordict,\n\u001b[1;32m   2627\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_cast_to_device\u001b[39m\u001b[38;5;124m\"\u001b[39m: auto_cast_to_device,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2632\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallback\u001b[39m\u001b[38;5;124m\"\u001b[39m: callback,\n\u001b[1;32m   2633\u001b[0m }\n\u001b[1;32m   2634\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m break_when_any_done \u001b[38;5;129;01mor\u001b[39;00m break_when_all_done:\n\u001b[0;32m-> 2635\u001b[0m     tensordicts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rollout_stop_early\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2636\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbreak_when_all_done\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbreak_when_all_done\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2637\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbreak_when_any_done\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbreak_when_any_done\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2638\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2639\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2640\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2641\u001b[0m     tensordicts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rollout_nonstop(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torchrl/envs/common.py:2722\u001b[0m, in \u001b[0;36mEnvBase._rollout_stop_early\u001b[0;34m(self, break_when_any_done, break_when_all_done, tensordict, auto_cast_to_device, max_steps, policy, policy_device, env_device, callback)\u001b[0m\n\u001b[1;32m   2719\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2720\u001b[0m         tensordict\u001b[38;5;241m.\u001b[39mclear_device_()\n\u001b[0;32m-> 2722\u001b[0m tensordict \u001b[38;5;241m=\u001b[39m \u001b[43mpolicy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensordict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2723\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m auto_cast_to_device:\n\u001b[1;32m   2724\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m env_device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensordict/nn/common.py:314\u001b[0m, in \u001b[0;36mdispatch.__call__.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m out\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _self \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 314\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_self\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensordict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(tensordict, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensordict/nn/utils.py:359\u001b[0m, in \u001b[0;36m_set_skip_existing_None.__call__.<locals>.wrapper\u001b[0;34m(_self, tensordict, *args, **kwargs)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprev \u001b[38;5;241m=\u001b[39m _SKIP_EXISTING\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 359\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_self\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensordict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    361\u001b[0m     _SKIP_EXISTING \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprev\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensordict/nn/common.py:1089\u001b[0m, in \u001b[0;36mTensorDictModule.forward\u001b[0;34m(self, tensordict, tensordict_out, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1087\u001b[0m in_keys \u001b[38;5;241m=\u001b[39m indent(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min_keys=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_keys\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m4\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1088\u001b[0m out_keys \u001b[38;5;241m=\u001b[39m indent(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mout_keys=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_keys\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m4\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1089\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m err \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1090\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTensorDictModule failed with operation\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmodule\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00min_keys\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mout_keys\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1091\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensordict/nn/common.py:1054\u001b[0m, in \u001b[0;36mTensorDictModule.forward\u001b[0;34m(self, tensordict, tensordict_out, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(tensor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m tensor \u001b[38;5;129;01min\u001b[39;00m tensors) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(err):\n\u001b[1;32m   1049\u001b[0m     none_set \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   1050\u001b[0m         key\n\u001b[1;32m   1051\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m key, tensor \u001b[38;5;129;01min\u001b[39;00m _zip_strict(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_keys, tensors)\n\u001b[1;32m   1052\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m tensor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m     }\n\u001b[0;32m-> 1054\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[1;32m   1055\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSome tensors that are necessary for the module call may \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1056\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot have not been found in the input tensordict: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1057\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe following inputs are None: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnone_set\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1058\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   1059\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1060\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Some tensors that are necessary for the module call may not have not been found in the input tensordict: the following inputs are None: {'observation'}.\""
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "pbar = tqdm.tqdm(range(4 // batch_size))\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optim, 4)\n",
    "logs = defaultdict(list)\n",
    "\n",
    "for _ in pbar:\n",
    "    init_td = env.reset(env.gen_params(batch_size=[batch_size]))\n",
    "    rollout = env.rollout(4, policy, tensordict=init_td, auto_reset=False)\n",
    "    traj_return = rollout[\"next\", \"reward\"].mean()\n",
    "    (-traj_return).backward()\n",
    "    gn = torch.nn.utils.clip_grad_norm_(net.parameters(), 1.0)\n",
    "    optim.step()\n",
    "    optim.zero_grad()\n",
    "    pbar.set_description(\n",
    "        f\"reward: {traj_return: 4.4f}, \"\n",
    "        f\"last reward: {rollout[..., -1]['next', 'reward'].mean(): 4.4f}, gradient norm: {gn: 4.4}\"\n",
    "    )\n",
    "    logs[\"return\"].append(traj_return.item())\n",
    "    logs[\"last_reward\"].append(rollout[..., -1][\"next\", \"reward\"].mean().item())\n",
    "    scheduler.step()\n",
    "\n",
    "\n",
    "def plot():\n",
    "    import matplotlib\n",
    "    from matplotlib import pyplot as plt\n",
    "\n",
    "    is_ipython = \"inline\" in matplotlib.get_backend()\n",
    "    if is_ipython:\n",
    "        from IPython import display\n",
    "\n",
    "    with plt.ion():\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(logs[\"return\"])\n",
    "        plt.title(\"returns\")\n",
    "        plt.xlabel(\"iteration\")\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(logs[\"last_reward\"])\n",
    "        plt.title(\"last reward\")\n",
    "        plt.xlabel(\"iteration\")\n",
    "        if is_ipython:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Troubleshott 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_composite_from_td(td):\n",
    "    # custom function to convert a ``tensordict`` in a similar spec structure\n",
    "    # of unbounded values.\n",
    "    composite = CompositeSpec(\n",
    "        {\n",
    "            key: make_composite_from_td(tensor)\n",
    "            if isinstance(tensor, TensorDictBase)\n",
    "            else UnboundedContinuousTensorSpec(\n",
    "                dtype=tensor.dtype, device=tensor.device, shape=tensor.shape\n",
    "            )\n",
    "            for key, tensor in td.items()\n",
    "        },\n",
    "        shape=td.shape,\n",
    "    )\n",
    "    return composite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_params(batch_size=None, *args, **kwargs) -> TensorDictBase:\n",
    "    \"\"\"Returns a tensordict containing the parameters with specified batch size.\"\"\"\n",
    "    if batch_size is None:\n",
    "        batch_size = [1]\n",
    "        \n",
    "    td = TensorDict(\n",
    "        {\n",
    "            \"params\": TensorDict(\n",
    "                {\n",
    "                    \"step_start\": torch.zeros(batch_size, dtype=torch.int32),\n",
    "                    \"step_end\": torch.full(batch_size, 20, dtype=torch.int32),\n",
    "                    \"max_linear_velocity\": torch.ones(batch_size, dtype=torch.float32),\n",
    "                    \"max_angular_velocity\": torch.ones(batch_size, dtype=torch.float32),\n",
    "                    \"imageHeight\": torch.full(batch_size, 480, dtype=torch.int32),\n",
    "                    \"imageWidth\": torch.full(batch_size, 640, dtype=torch.int32),\n",
    "                },\n",
    "                batch_size=batch_size,\n",
    "            )\n",
    "        },\n",
    "        batch_size=batch_size,\n",
    "    )\n",
    "    return td\n",
    "\n",
    "def _step(tensordict):\n",
    "    # Take a step\n",
    "    step = tensordict[\"stepInt\"]\n",
    "    batch_size = step.shape[0]\n",
    "    \n",
    "    # Create image with proper batch dimension\n",
    "    height, width, channels = 480, 640, 3\n",
    "    image = np.zeros((batch_size, height, width, channels), dtype=np.uint8)\n",
    "    image_tensor = torch.tensor(image, dtype=torch.float32)\n",
    "\n",
    "    # Ensure step increment maintains batch dimension\n",
    "    step = step + torch.ones_like(step)\n",
    "\n",
    "    # Initialize tensors with proper batch dimension\n",
    "    progress = torch.zeros((batch_size, 1), dtype=torch.float32)\n",
    "    reward = progress.clone()\n",
    "    done = torch.zeros((batch_size, 1), dtype=torch.bool)\n",
    "\n",
    "    out = TensorDict(\n",
    "        {\n",
    "            \"stepInt\": step,\n",
    "            \"image\": image_tensor,\n",
    "            \"params\": tensordict[\"params\"],\n",
    "            \"reward\": reward,\n",
    "            \"done\": done,\n",
    "        },\n",
    "        batch_size=[batch_size],\n",
    "    )\n",
    "\n",
    "    return out\n",
    "\n",
    "def _make_spec(self, td_params):\n",
    "    batch_size = td_params.batch_size\n",
    "\n",
    "    self.observation_spec = CompositeSpec(\n",
    "        stepInt=BoundedTensorSpec(\n",
    "            low=td_params[\"params\", \"step_start\"],\n",
    "            high=td_params[\"params\", \"step_end\"],\n",
    "            shape=batch_size,\n",
    "            dtype=torch.int32,\n",
    "        ),\n",
    "        image=UnboundedContinuousTensorSpec(\n",
    "            shape=(batch_size[0], td_params[\"params\", \"imageHeight\"][0].item(), \n",
    "                  td_params[\"params\", \"imageWidth\"][0].item(), 3),\n",
    "            dtype=torch.float32,\n",
    "        ),\n",
    "        params=make_composite_from_td(td_params[\"params\"]),\n",
    "        shape=batch_size,\n",
    "    )\n",
    "\n",
    "    self.action_spec = CompositeSpec(\n",
    "        action=CompositeSpec(\n",
    "            linear_velocity=BoundedTensorSpec(\n",
    "                low=-td_params[\"params\", \"max_linear_velocity\"],\n",
    "                high=td_params[\"params\", \"max_linear_velocity\"],\n",
    "                shape=batch_size,\n",
    "                dtype=torch.float32,\n",
    "            ),\n",
    "            angular_velocity=BoundedTensorSpec(\n",
    "                low=-td_params[\"params\", \"max_angular_velocity\"],\n",
    "                high=td_params[\"params\", \"max_angular_velocity\"],\n",
    "                shape=batch_size,\n",
    "                dtype=torch.float32,\n",
    "            ),\n",
    "            shape=batch_size,\n",
    "        ),\n",
    "        shape=batch_size,\n",
    "    )\n",
    "    self.reward_spec = UnboundedContinuousTensorSpec(shape=(batch_size[0], 1))\n",
    "\n",
    "def _reset(self, td_params=None):\n",
    "    if td_params is None:\n",
    "        td_params = self.gen_params()\n",
    "    \n",
    "    batch_size = td_params.batch_size\n",
    "    \n",
    "    return TensorDict({\n",
    "        \"stepInt\": torch.zeros(batch_size, dtype=torch.int32),\n",
    "        \"image\": torch.zeros((batch_size[0],\n",
    "                            td_params[\"params\", \"imageHeight\"][0].item(), \n",
    "                            td_params[\"params\", \"imageWidth\"][0].item(), 3), \n",
    "                           dtype=torch.float32),\n",
    "        \"params\": td_params[\"params\"]\n",
    "    }, batch_size=batch_size)\n",
    "\n",
    "class my_botEnv(EnvBase):\n",
    "    metadata = {\n",
    "        \"render_modes\": [\"human\", \"rgb_array\"],\n",
    "        \"render_fps\": 30,\n",
    "    }\n",
    "    batch_locked = False\n",
    "\n",
    "    def __init__(self, td_params=None, seed=None, device=\"cpu\", batch_size=None):\n",
    "        if batch_size is None:\n",
    "            batch_size = [1]\n",
    "            \n",
    "        if td_params is None:\n",
    "            td_params = self.gen_params(batch_size=batch_size)\n",
    "\n",
    "        super().__init__(device=device, batch_size=batch_size)\n",
    "        self._make_spec(td_params)\n",
    "        if seed is None:\n",
    "            seed = torch.empty((), dtype=torch.int64).random_().item()\n",
    "        self.set_seed(seed)\n",
    "\n",
    "    def _set_seed(self, seed: Optional[int]):\n",
    "        rng = torch.manual_seed(seed)\n",
    "        self.rng = rng\n",
    "\n",
    "    gen_params = staticmethod(gen_params)\n",
    "    _make_spec = _make_spec\n",
    "    _reset = _reset\n",
    "    _step = staticmethod(_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/torchrl/data/tensor_specs.py:5464: DeprecationWarning: The CompositeSpec has been deprecated and will be removed in v0.7. Please use Composite instead.\n",
      "  warnings.warn(\n",
      "2024-10-26 21:45:16,604 [torchrl][INFO] check_env_specs succeeded!\n"
     ]
    }
   ],
   "source": [
    "env = my_botEnv()\n",
    "check_env_specs(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation_spec: Composite(\n",
      "    stepInt: BoundedDiscrete(\n",
      "        shape=torch.Size([1]),\n",
      "        space=ContinuousBox(\n",
      "            low=Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.int32, contiguous=True),\n",
      "            high=Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.int32, contiguous=True)),\n",
      "        device=cpu,\n",
      "        dtype=torch.int32,\n",
      "        domain=discrete),\n",
      "    image: UnboundedContinuous(\n",
      "        shape=torch.Size([1, 480, 640, 3]),\n",
      "        space=ContinuousBox(\n",
      "            low=Tensor(shape=torch.Size([1, 480, 640, 3]), device=cpu, dtype=torch.float32, contiguous=True),\n",
      "            high=Tensor(shape=torch.Size([1, 480, 640, 3]), device=cpu, dtype=torch.float32, contiguous=True)),\n",
      "        device=cpu,\n",
      "        dtype=torch.float32,\n",
      "        domain=continuous),\n",
      "    params: Composite(\n",
      "        step_start: UnboundedDiscrete(\n",
      "            shape=torch.Size([1]),\n",
      "            space=ContinuousBox(\n",
      "                low=Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.int32, contiguous=True),\n",
      "                high=Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.int32, contiguous=True)),\n",
      "            device=cpu,\n",
      "            dtype=torch.int32,\n",
      "            domain=discrete),\n",
      "        step_end: UnboundedDiscrete(\n",
      "            shape=torch.Size([1]),\n",
      "            space=ContinuousBox(\n",
      "                low=Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.int32, contiguous=True),\n",
      "                high=Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.int32, contiguous=True)),\n",
      "            device=cpu,\n",
      "            dtype=torch.int32,\n",
      "            domain=discrete),\n",
      "        max_linear_velocity: UnboundedContinuous(\n",
      "            shape=torch.Size([1]),\n",
      "            space=ContinuousBox(\n",
      "                low=Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, contiguous=True),\n",
      "                high=Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, contiguous=True)),\n",
      "            device=cpu,\n",
      "            dtype=torch.float32,\n",
      "            domain=continuous),\n",
      "        max_angular_velocity: UnboundedContinuous(\n",
      "            shape=torch.Size([1]),\n",
      "            space=ContinuousBox(\n",
      "                low=Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, contiguous=True),\n",
      "                high=Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, contiguous=True)),\n",
      "            device=cpu,\n",
      "            dtype=torch.float32,\n",
      "            domain=continuous),\n",
      "        imageHeight: UnboundedDiscrete(\n",
      "            shape=torch.Size([1]),\n",
      "            space=ContinuousBox(\n",
      "                low=Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.int32, contiguous=True),\n",
      "                high=Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.int32, contiguous=True)),\n",
      "            device=cpu,\n",
      "            dtype=torch.int32,\n",
      "            domain=discrete),\n",
      "        imageWidth: UnboundedDiscrete(\n",
      "            shape=torch.Size([1]),\n",
      "            space=ContinuousBox(\n",
      "                low=Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.int32, contiguous=True),\n",
      "                high=Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.int32, contiguous=True)),\n",
      "            device=cpu,\n",
      "            dtype=torch.int32,\n",
      "            domain=discrete),\n",
      "        device=cpu,\n",
      "        shape=torch.Size([1])),\n",
      "    device=cpu,\n",
      "    shape=torch.Size([1]))\n",
      "state_spec: Composite(\n",
      ",\n",
      "    device=cpu,\n",
      "    shape=torch.Size([1]))\n",
      "reward_spec: UnboundedContinuous(\n",
      "    shape=torch.Size([1, 1]),\n",
      "    space=ContinuousBox(\n",
      "        low=Tensor(shape=torch.Size([1, 1]), device=cpu, dtype=torch.float32, contiguous=True),\n",
      "        high=Tensor(shape=torch.Size([1, 1]), device=cpu, dtype=torch.float32, contiguous=True)),\n",
      "    device=cpu,\n",
      "    dtype=torch.float32,\n",
      "    domain=continuous)\n"
     ]
    }
   ],
   "source": [
    "print(\"observation_spec:\", env.observation_spec)\n",
    "print(\"state_spec:\", env.state_spec)\n",
    "print(\"reward_spec:\", env.reward_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reset tensordict TensorDict(\n",
      "    fields={\n",
      "        done: Tensor(shape=torch.Size([1, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        image: Tensor(shape=torch.Size([1, 480, 640, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        params: TensorDict(\n",
      "            fields={\n",
      "                imageHeight: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.int32, is_shared=False),\n",
      "                imageWidth: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.int32, is_shared=False),\n",
      "                max_angular_velocity: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                max_linear_velocity: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                step_end: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.int32, is_shared=False),\n",
      "                step_start: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.int32, is_shared=False)},\n",
      "            batch_size=torch.Size([1]),\n",
      "            device=None,\n",
      "            is_shared=False),\n",
      "        stepInt: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.int32, is_shared=False),\n",
      "        terminated: Tensor(shape=torch.Size([1, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "    batch_size=torch.Size([1]),\n",
      "    device=None,\n",
      "    is_shared=False)\n"
     ]
    }
   ],
   "source": [
    "td = env.reset()\n",
    "print(\"reset tensordict\", td)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random step tensordict TensorDict(\n",
      "    fields={\n",
      "        action: TensorDict(\n",
      "            fields={\n",
      "                angular_velocity: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                linear_velocity: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
      "            batch_size=torch.Size([1]),\n",
      "            device=cpu,\n",
      "            is_shared=False),\n",
      "        done: Tensor(shape=torch.Size([1, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        image: Tensor(shape=torch.Size([1, 480, 640, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        next: TensorDict(\n",
      "            fields={\n",
      "                done: Tensor(shape=torch.Size([1, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                image: Tensor(shape=torch.Size([1, 480, 640, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                params: TensorDict(\n",
      "                    fields={\n",
      "                        imageHeight: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.int32, is_shared=False),\n",
      "                        imageWidth: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.int32, is_shared=False),\n",
      "                        max_angular_velocity: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                        max_linear_velocity: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                        step_end: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.int32, is_shared=False),\n",
      "                        step_start: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.int32, is_shared=False)},\n",
      "                    batch_size=torch.Size([1]),\n",
      "                    device=None,\n",
      "                    is_shared=False),\n",
      "                reward: Tensor(shape=torch.Size([1, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                stepInt: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.int32, is_shared=False),\n",
      "                terminated: Tensor(shape=torch.Size([1, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "            batch_size=torch.Size([1]),\n",
      "            device=None,\n",
      "            is_shared=False),\n",
      "        params: TensorDict(\n",
      "            fields={\n",
      "                imageHeight: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.int32, is_shared=False),\n",
      "                imageWidth: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.int32, is_shared=False),\n",
      "                max_angular_velocity: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                max_linear_velocity: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                step_end: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.int32, is_shared=False),\n",
      "                step_start: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.int32, is_shared=False)},\n",
      "            batch_size=torch.Size([1]),\n",
      "            device=None,\n",
      "            is_shared=False),\n",
      "        stepInt: Tensor(shape=torch.Size([1]), device=cpu, dtype=torch.int32, is_shared=False),\n",
      "        terminated: Tensor(shape=torch.Size([1, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "    batch_size=torch.Size([1]),\n",
      "    device=None,\n",
      "    is_shared=False)\n"
     ]
    }
   ],
   "source": [
    "td = env.rand_step(td)\n",
    "print(\"random step tensordict\", td)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_env = my_botEnv()\n",
    "transform = Compose(ToTensorImage(in_keys=[\"image\"]), Resize(64, 64, in_keys=[\"image\"]))\n",
    "env = TransformedEnv(base_env, transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-26 04:37:00,270 [torchrl][INFO] check_env_specs succeeded!\n"
     ]
    }
   ],
   "source": [
    "check_env_specs(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data from rollout: TensorDict(\n",
      "    fields={\n",
      "        action: TensorDict(\n",
      "            fields={\n",
      "                action: TensorDict(\n",
      "                    fields={\n",
      "                        angular_velocity: Tensor(shape=torch.Size([15, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                        linear_velocity: Tensor(shape=torch.Size([15, 1]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
      "                    batch_size=torch.Size([15, 1]),\n",
      "                    device=cpu,\n",
      "                    is_shared=False)},\n",
      "            batch_size=torch.Size([15, 1]),\n",
      "            device=cpu,\n",
      "            is_shared=False),\n",
      "        done: Tensor(shape=torch.Size([15, 1, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "        image: Tensor(shape=torch.Size([15, 1, 3, 64, 64]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "        next: TensorDict(\n",
      "            fields={\n",
      "                done: Tensor(shape=torch.Size([15, 1, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
      "                image: Tensor(shape=torch.Size([15, 1, 3, 64, 64]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                params: TensorDict(\n",
      "                    fields={\n",
      "                        imageHeight: Tensor(shape=torch.Size([15, 1]), device=cpu, dtype=torch.int32, is_shared=False),\n",
      "                        imageWidth: Tensor(shape=torch.Size([15, 1]), device=cpu, dtype=torch.int32, is_shared=False),\n",
      "                        max_angular_velocity: Tensor(shape=torch.Size([15, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                        max_linear_velocity: Tensor(shape=torch.Size([15, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                        step_end: Tensor(shape=torch.Size([15, 1]), device=cpu, dtype=torch.int32, is_shared=False),\n",
      "                        step_start: Tensor(shape=torch.Size([15, 1]), device=cpu, dtype=torch.int32, is_shared=False)},\n",
      "                    batch_size=torch.Size([15, 1]),\n",
      "                    device=None,\n",
      "                    is_shared=False),\n",
      "                reward: Tensor(shape=torch.Size([15, 1, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                stepInt: Tensor(shape=torch.Size([15, 1]), device=cpu, dtype=torch.int32, is_shared=False),\n",
      "                terminated: Tensor(shape=torch.Size([15, 1, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "            batch_size=torch.Size([15, 1]),\n",
      "            device=None,\n",
      "            is_shared=False),\n",
      "        params: TensorDict(\n",
      "            fields={\n",
      "                imageHeight: Tensor(shape=torch.Size([15, 1]), device=cpu, dtype=torch.int32, is_shared=False),\n",
      "                imageWidth: Tensor(shape=torch.Size([15, 1]), device=cpu, dtype=torch.int32, is_shared=False),\n",
      "                max_angular_velocity: Tensor(shape=torch.Size([15, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                max_linear_velocity: Tensor(shape=torch.Size([15, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
      "                step_end: Tensor(shape=torch.Size([15, 1]), device=cpu, dtype=torch.int32, is_shared=False),\n",
      "                step_start: Tensor(shape=torch.Size([15, 1]), device=cpu, dtype=torch.int32, is_shared=False)},\n",
      "            batch_size=torch.Size([15, 1]),\n",
      "            device=None,\n",
      "            is_shared=False),\n",
      "        stepInt: Tensor(shape=torch.Size([15, 1]), device=cpu, dtype=torch.int32, is_shared=False),\n",
      "        terminated: Tensor(shape=torch.Size([15, 1, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
      "    batch_size=torch.Size([15]),\n",
      "    device=None,\n",
      "    is_shared=False)\n"
     ]
    }
   ],
   "source": [
    "def simple_rollout(steps=15):\n",
    "    # preallocate:\n",
    "    data = TensorDict({}, [steps])\n",
    "    # reset\n",
    "    _data = env.reset()\n",
    "    for i in range(steps):\n",
    "        _data[\"action\"] = env.action_spec.rand()\n",
    "        _data = env.step(_data)\n",
    "        data[i] = _data\n",
    "        _data = step_mdp(_data, keep_other=True)\n",
    "    return data\n",
    "\n",
    "\n",
    "print(\"data from rollout:\", simple_rollout(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1 # number of environments to be executed in batch\n",
    "td = env.reset(env.gen_params(batch_size=[batch_size]))\n",
    "print(f\"reset (batch size of {batch_size})\", td)\n",
    "td = env.rand_step(td)\n",
    "print(f\"rand step (batch size of {batch_size})\", td)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RL Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from tensordict import TensorDict, TensorDictBase\n",
    "from torchrl.envs import EnvBase\n",
    "from torchrl.envs.utils import check_env_specs\n",
    "from torchrl.data import CompositeSpec, BoundedTensorSpec, UnboundedContinuousTensorSpec\n",
    "import torch.nn as nn\n",
    "\n",
    "class ImageBasedRLAgent:\n",
    "    def __init__(self, batch_size=None, device=\"cpu\", seed=None):\n",
    "        \"\"\"\n",
    "        Initialize the RL agent with the environment and policy network.\n",
    "        \n",
    "        Args:\n",
    "            batch_size (list): Batch size for processing, defaults to [1]\n",
    "            device (str): Device to run computations on\n",
    "            seed (int): Random seed for reproducibility\n",
    "        \"\"\"\n",
    "        self.device = device\n",
    "        self.batch_size = [1] if batch_size is None else batch_size\n",
    "        \n",
    "        # Initialize the environment\n",
    "        self.env = my_botEnv(batch_size=self.batch_size, device=self.device, seed=seed)\n",
    "        \n",
    "        # Initialize the policy network\n",
    "        self.policy_net = PolicyNetwork().to(device)\n",
    "        \n",
    "    def preprocess_image(self, image):\n",
    "        \"\"\"\n",
    "        Preprocess the input image for the policy network.\n",
    "        \n",
    "        Args:\n",
    "            image (torch.Tensor): Input image tensor of shape (B, H, W, C)\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Preprocessed image tensor\n",
    "        \"\"\"\n",
    "        # Normalize pixel values to [0, 1]\n",
    "        image = image.float() / 255.0\n",
    "        \n",
    "        # Permute dimensions from (B, H, W, C) to (B, C, H, W)\n",
    "        image = image.permute(0, 3, 1, 2)\n",
    "        \n",
    "        return image.to(self.device)\n",
    "    \n",
    "    def select_action(self, image, step_count):\n",
    "        \"\"\"\n",
    "        Select an action based on the current observation.\n",
    "        \n",
    "        Args:\n",
    "            image (torch.Tensor): Current observation image\n",
    "            step_count (torch.Tensor): Current step count\n",
    "            \n",
    "        Returns:\n",
    "            TensorDict: Selected action in the required format\n",
    "        \"\"\"\n",
    "        # Preprocess image\n",
    "        processed_image = self.preprocess_image(image)\n",
    "        \n",
    "        # Get policy network predictions\n",
    "        with torch.no_grad():\n",
    "            linear_vel, angular_vel = self.policy_net(processed_image)\n",
    "        \n",
    "        # Create action TensorDict\n",
    "        action_dict = TensorDict({\n",
    "            \"action\": TensorDict({\n",
    "                \"linear_velocity\": linear_vel,\n",
    "                \"angular_velocity\": angular_vel\n",
    "            }, batch_size=self.batch_size)\n",
    "        }, batch_size=self.batch_size)\n",
    "        \n",
    "        return action_dict\n",
    "    \n",
    "    def update_policy(self, reward):\n",
    "        \"\"\"\n",
    "        Update the policy based on the received reward.\n",
    "        This is a placeholder for implementing actual policy updates.\n",
    "        \n",
    "        Args:\n",
    "            reward (torch.Tensor): Reward received from the environment\n",
    "        \"\"\"\n",
    "        # Implement your policy update logic here\n",
    "        pass\n",
    "\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural network for processing images and outputting actions.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # CNN layers for processing images\n",
    "        self.conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Fully connected layers for action output\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(64 * 53 * 73, 512),  # Adjusted for 480x640 input\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 2)  # 2 outputs: linear and angular velocity\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Input image tensor of shape (B, C, H, W)\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (linear_velocity, angular_velocity) tensors\n",
    "        \"\"\"\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.reshape(x.size(0), -1)  # Flatten\n",
    "        x = self.fc_layers(x)\n",
    "        \n",
    "        # Split output into linear and angular velocity\n",
    "        linear_velocity = x[:, 0].unsqueeze(-1)\n",
    "        angular_velocity = x[:, 1].unsqueeze(-1)\n",
    "        \n",
    "        return linear_velocity, angular_velocity\n",
    "\n",
    "# Example usage\n",
    "def main():\n",
    "    # Initialize the agent\n",
    "    agent = ImageBasedRLAgent()\n",
    "    \n",
    "    # Reset the environment\n",
    "    tensordict = agent.env.reset()\n",
    "    \n",
    "    # Run a few steps\n",
    "    for _ in range(5):\n",
    "        # Get the current image and step count\n",
    "        current_image = tensordict[\"image\"]\n",
    "        step_count = tensordict[\"stepInt\"]\n",
    "        \n",
    "        # Select action\n",
    "        action = agent.select_action(current_image, step_count)\n",
    "        \n",
    "        # Take a step in the environment\n",
    "        tensordict = agent.env.step(action)\n",
    "        \n",
    "        # Update policy (if implementing learning)\n",
    "        if \"reward\" in tensordict:\n",
    "            agent.update_policy(tensordict[\"reward\"])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
